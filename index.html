<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Anna Huang</title>
    <meta name="author" content="Anna Huang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YM02RY2PZY"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-YM02RY2PZY');
    </script>
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">

              <td style="padding:1%;width:35%;max-width:35%">
                <img style="text-align:center;width:100%;max-width:80%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/anna1-photo.jpeg">
              
                <p style="text-align:center;max-width:80%">
                  &nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/citations?user=NRz_EVgAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/huangcza">Twitter</a>
                </p>

              <td style="padding:20px;width:70%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                    Cheng-Zhi Anna Huang  ÈªÉÊàê‰πã
                </p>

                <p>In Fall 2024, I started a faculty position at <a href="https://web.mit.edu/">Massachusetts Institute of Technology</a> (MIT), with a shared position between <a href="https://www.eecs.mit.edu/">Electrical Engineering and Computer Science</a> (EECS) and <a href="https://mta.mit.edu/">Music and Theater Arts</a> (MTA). 
                For the past 8 years, I have been a researcher at <a href="https://magenta.tensorflow.org/">Magenta</a> in Google Brain and then <a href="https://deepmind.google/">Google DeepMind</a>, working on generative models and interfaces to support human-AI partnerships in music making. 
                </p>
                <p>
                I am the creator of the ML model Coconet that powered Google‚Äôs first AI Doodle, the <a href="https://magenta.tensorflow.org/coconet">Bach Doodle</a>. In two days, Coconet harmonized <strong><em>55 million</em></strong> melodies from users around the world. In 2018, I created <a href="https://magenta.tensorflow.org/music-transformer">Music Transformer</a>, a breakthrough in generating music with long-term structure, and the first successful adaptation of the transformer architecture to music.  Our ICLR paper is currently the <strong>most cited paper</strong> in music generation. 
                </p>
                <p>
                I was a <a href="https://cifar.ca/bios/cheng-zhi-anna-huang/">Canada CIFAR AI Chair</a> at <a href="https://mila.quebec/en/">Mila</a>, and continue to hold an adjunct professorship at <a href="https://diro.umontreal.ca/english/home/">University of Montreal</a>. I was a judge then organizer for <a href="https://www.aisongcontest.com/the-2023-finalists">AI Song Contest</a> 2020-22.
                I did my PhD at Harvard University, master‚Äôs at the MIT Media Lab, and a dual bachelor‚Äôs at University of Southern California in music composition and CS. 
                </p>
              </td> 
            </tr>
          </tbody></table>
          

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         <tr>
          <td style="padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
          <div style="background-color: #f6f6f6; padding:.5em 1.5em .5em;">
          <p> üéµ
	     üéµ We have multiple Postdoc positions open for Fall 2025 at <a href="https://musictech.mit.edu/postdoctoral-opportunities/">MIT Music Technology</a>, 
	     through the <a href="https://shass.mit.edu/">School of Humanities, Arts, and Social Sciences (SHASS)</a> 
	     as well as the <a href=‚Äùhttps://computing.mit.edu/‚Äù>College of Computing</a>. &nbsp;&nbsp;
	     
	     üéµ For PhD, apply through MIT <a href="https://www.eecs.mit.edu/academics/graduate-programs/admission-process/">EECS</a> (by Dec 1). 
	     See <a href="#recruiting">recruiting</a> section below for more details.&nbsp; 
          </p>

          </div>
          <br>
          </td>
         </tr>
         <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Research Interests</h2>
            <p>
            I‚Äôm interested in taking an <strong>interaction-driven</strong> approach to designing <strong>Generative AI</strong>, to enable <strong>new ways of interacting with music</strong> (and AI) that can extend how we understand, learn, and create music. I aim to partner with musicians, to design for the specificity of their creative practice and tradition, which inevitably invites new ways of thinking about <strong>generative modeling</strong> and <strong>Human-AI collaboration</strong>.
            </p>
            <p>
            I propose to use neural networks (NNs) as a lens onto music, and a mirror onto our own understanding of music. I‚Äôm interested in <strong>music theories</strong> and <strong>music cognition</strong> <i>of</i> NNs and <i>for</i> NNs, to understand, regularize and calibrate their musical behaviors. I aim to work towards <strong>interpretability</strong> and <strong>explainability</strong> that is useful for musicians interacting with the AI system. I envision <strong>working with musicians to design interactive systems and visualizations</strong> that empower them to understand, debug, steer, and align the generative AI‚Äôs behavior.
            </p>
            <p>
            I‚Äôm also interested in rethinking generative AI through the lens of <strong>social reinforcement learning</strong> (RL) and <strong>multi-agent RL</strong>, to elicit <strong>creativity</strong> not through imitation but <strong>through interaction</strong>. This framework invites us to consider how <strong>game design</strong> and <strong>reward modeling</strong> can influence how agents and users interact. I envision a <strong>jam space</strong>, where musicians and agents can jam together, and researchers can swap in their own generative agents and reward models, similar to <strong>OpenAI‚Äôs Gym</strong>. The evaluation is not only on the resulting music, but also on the interactions, how well agents support other players. I‚Äôm also interested in <strong>efficient machine learning</strong>, to build instruments and agents that can run in <strong>real-time</strong>, to enable <strong>Human-AI collective improvisation</strong>.
            </p>
            </td>
          </tr> 
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <colgroup>
              <col class="colone">
              <col class="coltwo">
            </colgroup>
            <tbody>

<tr> 
  <td class="colleft">
    <div class="one">
      <img src='images/music-transformer-cropped.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://magenta.tensorflow.org/music-transformer">
      <span class="papertitle">Music Transformer</span>
    </a><br>
	  <strong>Cheng-Zhi Anna Huang</strong>, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, Douglas Eck. <em>ICLR, 2019</em><br>
   
  </td>
</tr>   

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/bach-doodle.jpeg'>
    </div>
  </td>
  <td class="colright">
    
    <a href="https://magenta.tensorflow.org/coconet">
      <span class="papertitle">The Bach Doodle: Approachable Music Composition with Machine Learning at Scale</span>
    </a><br>
    <strong>Cheng-Zhi Anna Huang</strong>, Curtis Hawthorne, Adam Roberts, Monica Dinculescu, James Wexler, Leon Hong, Jacob Howcroft. <em>ISMIR, 2019</em>
    <br>
   
  </td>
</tr>   

<tr> 
  <td class="colleft">
    <div class="one">
      <img src='images/gibbs.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://magenta.tensorflow.org/coconet">
      <span class="papertitle">Coconet: Counterpoint by Convolution</span>
    </a><br>
    <strong>Cheng-Zhi Anna Huang</strong>, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. <em>ISMIR, 2017</em><br>
  </td>
</tr>  



<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/ai-song-contest.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://magenta.tensorflow.org/aisongcontest">
      <span class="papertitle">AI Song Contest: Human-AI Co-Creation in Songwriting</span>
    </a><br>
    <strong>Cheng-Zhi Anna Huang</strong>, Hendrik Vincent Koops, Ed Newton-Rex, Monica Dinculescu, Carrie J Cai. <em>ISMIR, 2020</em><br>
   
  </td>
</tr>  

<tr> 
  <td class="colleft">
    <div class="one">
      <img src='images/mididdsp2.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://magenta.tensorflow.org/midi-ddsp">
      <span class="papertitle">MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling</span>
    </a><br>
    Yusong Wu, Ethan Manilow, Yi Deng, Rigel Swavely, Kyle Kastner, Tim Cooijmans, Aaron Courville, <strong>Cheng-Zhi Anna Huang</strong>, Jesse Engel. <em>ICLR, 2022</em><br>
    <br>
     <a href="https://ctrlgenworkshop.github.io/accepted_papers.html">Outstanding Paper Award</a> at<em> NeurIPS Workshop CtrlGen: Controllable Generative Modeling in Language and Vision, 2021</em>
   
   
  </td>
</tr> 

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/expressive-communication.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://dl.acm.org/doi/fullHtml/10.1145/3490099.3511159">
      <span class="papertitle">Expressive Communication: A Common Framework for Evaluating Developments in Generative Models and Steering Interfaces</span>
    </a><br>
    Ryan Louie, Jesse Engel, <strong>Cheng-Zhi Anna Huang</strong>. <em>IUI, 2022</em>. <a href="https://magenta.tensorflow.org/people-first-hci-ml-collaborations">Blog</a><br>
  </td>
</tr>  


<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/TISMIR2.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://transactions.ismir.net/articles/10.5334/tismir.129">
      <span class="papertitle">Editorial for TISMIR Special Collection: AI and Musical Creativity</span>
    </a><br>
    Bob LT Sturm, Alexandra L Uitdenbogerd, Hendrik Vincent Koops, <strong>Cheng-Zhi Anna Huang</strong>. <em>TISMIR, 2021</em><br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/steering.png'>
    </div>
  </td>
  <td class="colright">
    <a href="http://ceur-ws.org/Vol-3124/paper7.pdf">
      <span class="papertitle">Compositional Steering of Music Transformers</span>
    </a><br>
     Halley Young, Vincent Dumoulin, Pablo S Castro, Jesse Engel, <strong>Cheng-Zhi Anna Huang</strong><em>. HAI-GEN Workshop @ IUI, 2021</em><br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/orderless.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://arxiv.org/abs/2203.15140">
      <span class="papertitle">Improving Source Separation by Explicitly Modeling Dependencies Between Sources</span>
    </a><br>
    Ethan Manilow, Curtis Hawthorne, <strong>Cheng-Zhi Anna Huang</strong>, Bryan Pardo, Jesse Engel. <em>ICASSP, 2021</em><br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/cococo.jpeg'>
    </div>
  </td>
  <td class="colright">
    <a href="https://dl.acm.org/doi/fullHtml/10.1145/3313831.3376739">
      <span class="papertitle">Cococo: Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models</span>
    </a><br>
    Ryan Louie, Andy Coenen, <strong>Cheng-Zhi Anna Huang</strong>, Michael Terry, Carrie J Cai. <em>CHI, 2020</em><br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/maestro.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://magenta.tensorflow.org/maestro-wave2midi2wave">
      <span class="papertitle">Wave2Midi2Wave: Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset</span>
    </a><br>
    Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, <strong>Cheng-Zhi Anna Huang</strong>, Sander Dieleman, Erich Elsen, Jesse Engel, Douglas Eck. <em>ICLR, 2019</em><br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/infilling.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://nips2018creativity.github.io/doc/infilling_piano_performances.pdf">
      <span class="papertitle">Infilling Piano Performances</span>
    </a><br>
    Daphne Ippolito, <strong>Cheng-Zhi Anna Huang</strong>, Curtis Hawthorne, Douglas Eck. <em>NeurIPS Workshop on Machine Learning for Creativity and Design, 2018</em><br>
    <br>
    <a href="https://nips2018creativity.github.io/doc/Transformer_NADE.pdf">
      <span class="papertitle">Transformer-NADE for Piano Perofrmances</span>
    </a><br>
    Curtis Hawthorne, <strong>Cheng-Zhi Anna Huang</strong>, Daphne Ippolito, Douglas Eck. <em>NeurIPS Workshop on Machine Learning for Creativity and Design, 2018</em><br>
  </td>
</tr>

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/chordripple.png' width=160px>
    </div>
  </td>
  <td class="colright">
    <a href="https://www.eecs.harvard.edu/~kgajos/papers/2016/huang16chordripple.shtml
">
      <span class="papertitle">Chordripple: Recommending Chords to Help Novice Composers Go Beyond the Ordinary</span>
    </a><br>
    <strong>Cheng-Zhi Anna Huang</strong>, David Duvenaud, Krzysztof Z Gajos. <em>IUI, 2016</em><br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/knobs.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://www.eecs.harvard.edu/~kgajos/papers/2014/huang14active.shtml">
      <span class="papertitle">Active Learning of Intuitive Control Knobs for Synthesizers using Gaussian Processes</span>
    </a><br>
     <strong>Cheng-Zhi Anna Huang</strong>, David Duvenaud, Kenneth C Arnold, Brenton Partridge, Josiah W Oberholtzer, Krzysztof Z Gajos. <em>IUI, 2014</em><br>
  </td>
</tr>  

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/cross.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://dspace.mit.edu/handle/1721.1/46582">
      <span class="papertitle">Melodic Variations: Toward Cross-Cultural Transformation</span>
    </a><br>
     <strong>Cheng-Zhi Anna Huang</strong>. <em>Master's Thesis, MIT Media Lab, 2008</em><br>
  </td>
</tr> 

<tr>
  <td class="colleft">
    <div class="one">
      <img src='images/palestrina-pal.png'>
    </div>
  </td>
  <td class="colright">
    <a href="https://www.researchgate.net/profile/Elaine-Chew-3/publication/228788093_Palestrina_Pal_a_grammar_checker_for_music_compositions_in_the_style_of_Palestrina/links/02e7e51dac826a3c00000000/Palestrina-Pal-a-grammar-checker-for-music-compositions-in-the-style-of-Palestrina.pdf">
      <span class="papertitle">Palestrina Pal: a Grammar Checker for Music Compositions in the Style of Palestrina</span>
    </a><br>
     <strong>Cheng-Zhi Anna Huang</strong>, Elaine Chew. <em>Conference on Understanding and Creating Music, 2005</em><br>
  </td>
</tr>  
</tbody>
</table>


<h2 style="padding-top:40px;padding-left:20px;padding-bottom:20px;">Music Compositions</h2>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <colgroup>
    <col class="colone">
    <col class="coltwo">
  </colgroup>
  <tbody>
<tr> 
  <td class="colleft">
    <iframe width="100%" height="135" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/190544445&color=%23ff9900&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&visual=false"></iframe>
  </td>
  <td class="colright">
      <span class="papertitle">Searching</span><br>
      gu-zheng, violin, piano. 4'42". <em>2009</em><br>
      <em>a sight-reading recording, played by <strong>Cheng-Zhi Anna Huang</strong>, Chad Cannon, Max Hume</em><br>
  </td>
</tr>

<tr bgcolor="FBFBFB"> 
  <td class="colleft">
    <iframe width="100%" height="135" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1002046096&color=%23ff9900&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&visual=false"></iframe>
  </td>
  <td class="colright">
      <span class="papertitle">Fuguenza</span><br>
      two clarinets, bass clarinet. 2'10". <em>2005</em><br>
      <em>performed by Timothy Dodge, Raymond Santos, Andrew Leonard</em><br>
  </td>
</tr>

<tr> 
  <td class="colleft">
    <iframe width="100%" height="135" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/218843211&color=%23ff9900&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&visual=false"></iframe>
  </td>
  <td class="colright">
      <span class="papertitle">Breathe</span><br>
      8 divisi a cappella. 4'38". <em>2005</em><br>
      <em><a href='https://www.sfca.org/new-voices-project'>First Prize</a> in <a href="https://www.sfca.org/">San Francisco Choral Artists (SFCA)</a> New Voices Project. Recording by SFCA</em><br>
  </td>
</tr>

<tr bgcolor="FBFBFB"> 
  <td class="colleft">
    <iframe width="100%" height="135" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/218844631&color=%23ff9900&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&visual=false"></iframe>
  </td>
  <td class="colright">
      <span class="papertitle">The Butterfly Effect</span><br>
      flute, violin, basson, double bass. 3'50". <em>2004</em><br>
      <em>performed by Cathy Cho, Yen-Ping Lai, Marat Khusaenov, Brian Marrow</em><br>
  </td>
</tr>

<tr> 
  <td class="colleft">
    <iframe width="100%" height="135" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/140610125&color=%23ff9900&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&visual=false"></iframe>
  </td>
  <td class="colright">
      <span class="papertitle">Half-awake</span><br>
      stereo tape. 4'57". <em>2009</em><br>
      <em>the track is soft. listen with good headphones if possible.</em><br>
  </td>
</tr>

<tr bgcolor="FBFBFB"> 
  <td class="colleft">
    <iframe width="100%" height="135" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/148946279&color=%23ff9900&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&visual=false"></iframe>
  </td>
  <td class="colright">
      <span class="papertitle">A Lay of Sorrow</span><br>
      mezzo soprano, clarinet. 1'29". <em>2004</em><br>
      <em>performed by Angela Vincente, Andrew Leonard</em><br>
  </td>
</tr>

<tr> 
  <td class="colleft">
    <iframe width="100%" height="135" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/148946281&color=%23ff9900&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&visual=false"></iframe>
  </td>
  <td class="colright">
      <span class="papertitle">Beautiful Soup</span><br>
      mezzo soprano, clarinet. 1'42". <em>2004</em><br>
      <em>performed by Angela Vincente, Andrew Leonard</em><br>
  </td>
</tr>
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
         
    <div id="recruiting">  
      <h2 style="padding-top:40px;">Recruiting</h2>
    </div>

    <p>We have multiple Postdoc positions open for Fall 2025 at <a href="https://musictech.mit.edu/postdoctoral-opportunities/">MIT Music Technology</a>.  
	    I'm particularly interested in
    </p>
    If you are interested in working with me as a <strong>PhD student</strong>, consider applying to <a href="https://www.eecs.mit.edu/academics/graduate-programs/admission-process/">MIT EECS</a> (by December 1st). In the application <a href="https://gradapply.mit.edu/eecs">form</a>, you can find my name ("Huang, Anna") in the "potential research advisor" dropdown menu. <br>
    </p>
    <p>
    For indicating "research field of interest" in the application form, if we don't have music technology included in the dropdown menu yet, you can choose any field you're most interested in, such as (but not limited to) ML General Interests, Natural Language and Speech Processing, Reinforcement Learning, Deep Learning, Human Computer Interaction, or Cognitive AI, etc.<br>
    </p>
    <p>
    We'll reach out if additional information is needed. You'll probably hear back from MIT in February. Best of luck!
    </p>  
  </td>
</tr>

</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:20px">
      <br>
      <p style="font-size:small;">
        This website was built thanks to the help of <a href="https://github.com/jonbarron/jonbarron_website">this</a> source code.
      </p>
    </td>
  </tr>
</tbody></table>
</td>
</tr>
</table>
  </body>
</html>
